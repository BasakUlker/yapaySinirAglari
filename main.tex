\documentclass[11pt]{article}

% Language setting
\usepackage[turkish]{babel}
\usepackage{pythonhighlight}

\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=2cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{verbatim}
\usepackage{fancyhdr} % for header and footer
\usepackage{titlesec}
\usepackage{parskip}






\setlength{\parindent}{0pt}

\titleformat{\subsection}[runin]{\bfseries}{\thesubsection}{1em}{}

\pagestyle{fancy} % activate the custom header/footer

% define the header/footer contents
\lhead{\small{23BLM-4014 Yapay Sinir Ağları Ara Sınav Soru ve Cevap Kağıdı}}
\rhead{\small{Dr. Ulya Bayram}}
\lfoot{}
\rfoot{}

% remove header/footer on first page
\fancypagestyle{firstpage}{
  \lhead{}
  \rhead{}
  \lfoot{}
  \rfoot{\thepage}
}
 

\title{Çanakkale Onsekiz Mart Üniversitesi, Mühendislik Fakültesi, Bilgisayar Mühendisliği Akademik Dönem 2022-2023\\
Ders: BLM-4014 Yapay Sinir Ağları/Bahar Dönemi\\ 
ARA SINAV SORU VE CEVAP KAĞIDI\\
Dersi Veren Öğretim Elemanı: Dr. Öğretim Üyesi Ulya Bayram}
\author{%
\begin{minipage}{\textwidth}
\raggedright
Öğrenci Adı Soyadı: Başak Ülker\\ % Adınızı soyadınızı ve öğrenci numaranızı noktaların yerine yazın
Öğrenci No: 180401058
\end{minipage}%
}

\date{14 Nisan 2023}

\begin{document}
\maketitle

\vspace{-.5in}
\section*{Açıklamalar:}
\begin{itemize}
    \item Vizeyi çözüp, üzerinde aynı sorular, sizin cevaplar ve sonuçlar olan versiyonunu bu formatta PDF olarak, Teams üzerinden açtığım assignment kısmına yüklemeniz gerekiyor. Bu bahsi geçen PDF'i oluşturmak için LaTeX kullandıysanız, tex dosyasının da yer aldığı Github linkini de ödevin en başına (aşağı url olarak) eklerseniz bonus 5 Puan! (Tavsiye: Overleaf)
    \item Çözümlerde ya da çözümlerin kontrolünü yapmada internetten faydalanmak, ChatGPT gibi servisleri kullanmak serbest. Fakat, herkesin çözümü kendi emeğinden oluşmak zorunda. Çözümlerinizi, cevaplarınızı aşağıda belirttiğim tarih ve saate kadar kimseyle paylaşmayınız. 
    \item Kopyayı önlemek için Github repository'lerinizin hiçbirini \textbf{14 Nisan 2023, saat 15:00'a kadar halka açık (public) yapmayınız!} (Assignment son yükleme saati 13:00 ama internet bağlantısı sorunları olabilir diye en fazla ekstra 2 saat daha vaktiniz var. \textbf{Fakat 13:00 - 15:00 arası yüklemelerden -5 puan!}
    \item Ek puan almak için sağlayacağınız tüm Github repository'lerini \textbf{en geç 15 Nisan 2023 15:00'da halka açık (public) yapmış olun linklerden puan alabilmek için!}
    \item \textbf{14 Nisan 2023, saat 15:00'dan sonra gönderilen vizeler değerlendirilmeye alınmayacak, vize notu olarak 0 (sıfır) verilecektir!} Son anda internet bağlantısı gibi sebeplerden sıfır almayı önlemek için assignment kısmından ara ara çözümlerinizi yükleyebilirsiniz yedekleme için. Verilen son tarih/saatte (14 Nisan 2023, saat 15:00) sistemdeki en son yüklü PDF geçerli olacak.
    \item Çözümlerin ve kodların size ait ve özgün olup olmadığını kontrol eden bir algoritma kullanılacaktır. Kopya çektiği belirlenen vizeler otomatikman 0 (sıfır) alacaktır. Bu nedenle çözümlerinizi ve kodlarınızı yukarıda sağladığım gün ve saatlere kadar kimseyle paylaşmayınız.
    \item Bu vizeden alınabilecek en yüksek not 100'dür. Toplam aldığınız puan 100'ü geçerse, aldığınız not 100'e sabitlenecektir.
    \item LaTeX kullanarak PDF oluşturanlar öz geçmişlerine LaTeX bildiklerini de eklemeyi unutmasınlar :)
    \item Bu vizedeki soruların çözümleri ve tex dosyası için istediğiniz kadar sayıda Github repository'si oluşturabilirsiniz. Sadece yukarıda belirttiğim tarihlerde (14 Nisan 2023 saat 15:00 ile 15 Nisan 2023 saat 15:00 arasında) public yapmayı/halka açmayı ve Github profilinizi de öz geçmişinize eklemeyi unutmayın :)
    \item Bilgisayarınıza hiçbir program kurmadan, Overleaf, Google Colab, Kaggle gibi ortamlar üzerinden bu vizeyi tamamlamanız mümkün. İyi çalışmalar!!
\end{itemize}

(Ekstra 5 puan) \url{https://github.com/BasakUlker/yapaySinirAglari/edit/main/main.tex}

\newpage
\section{(Toplam 10 Puan) Hopfield Networks:}

\subsection{(2 Puan)} \textbf{ChatGPT’ye size Hopfield Network’ler ile ilgili detaylı bilgi vermesini isteyin. Sorduğunuz soruyu, ChatGPT'nin çıkartığı cevabı aşağı koyun. Bu cevapta anlamadığınız terimleri de ChatGPT’ye sorup, sorduğunuz soruları ve ChatGPT’nin verdiği cevapları da sırayla aşağı koyun.}

ChatGPT'nin cevabı:

Elbette! Hopfield ağı, geri beslemeli bir sinir ağıdır ve bir dizi çıkış düğümüne sahiptir. Her düğüm, diğer tüm düğümlerle bağlantılıdır ve her bir bağlantı bir ağırlıkla ağırlıklandırılmıştır. Bu ağırlıklar, ağın öğrenmesi sırasında belirlenir.

Hopfield ağları, özellikle hafıza ve tanıma sistemlerinde kullanılan modellerdir. Bu ağlar, öğrenme sırasında öğrenilen örüntüleri saklar ve ardından bu örüntüleri yeniden tanımak için kullanılabilir.

Hopfield ağları, asenkron veya senkron güncelleme yöntemleriyle çalışabilir. Asenkron güncelleme, ağın bir düğümünün sadece kendisinin güncellenmesini gerektirirken, senkron güncelleme tüm düğümlerin aynı anda güncellenmesini gerektirir. Senkron güncelleme, asenkron güncellemeye göre daha yavaş olsa da, ağın daha istikrarlı bir şekilde çalışmasını sağlar.

Hopfield ağları, özellikle hatırlama sorunları için kullanışlıdır. Örneğin, bir Hopfield ağına bir dizi öğrenilen örüntü verilir ve ardından ağa benzer bir örüntü sunulduğunda, ağ öğrenilmiş örüntüye en yakın olan örüntüyü hatırlar. Hopfield ağları ayrıca, öğrenilmiş örüntülerin bir parçasının kaybedilmesi durumunda bile kurtarma yeteneğine sahiptir.

Hopfield ağları, klasik yapay sinir ağlarına kıyasla daha az karmaşıktır ve matematiksel olarak daha iyi anlaşılırlar. Bununla birlikte, bu ağların büyük boyutlu problemlerde performansı düşüktür ve aynı anda birçok öğrenilmiş örüntü saklama kapasiteleri sınırlıdır.

Soru 1: Örüntü saklama kapasitelerini arttırmak mümkün mü? Mümkünse nasıl yapılabilir?

Hopfield ağlarının örüntü saklama kapasitesi, ağın boyutu ve öğrenilmiş örüntülerin karmaşıklığına bağlıdır. Örüntü saklama kapasitesi ağın büyüklüğü ile doğrusal olarak artarken, ağın hafıza kapasitesi öğrenilen örüntülerin karmaşıklığıyla ters orantılı olarak azalır.

Örüntü saklama kapasitesini artırmak için, Hopfield ağlarına alternatif öğrenme algoritmaları veya ağırlık güncelleme stratejileri uygulanabilir. Örneğin, genişletilmiş Hopfield ağı (EHNN) gibi bazı varyantlar, ağın öğrenme ve hatırlama yeteneğini artırmak için ek özellikler veya katmanlar ekler.

Bununla birlikte, örüntü saklama kapasitesi artırılırken, ağın kararlılığı da düşebilir ve öğrenilmiş örüntülerin çakışması sorunu ortaya çıkabilir. Bu nedenle, örüntü saklama kapasitesini artırma girişimleri, ağın doğru şekilde ölçeklendirilmesini ve dengelenmesini gerektirir.

Sonuç olarak, Hopfield ağlarının örüntü saklama kapasitesini artırmak mümkündür ancak bu, ağın boyutu, öğrenme algoritmaları ve güncelleme stratejileri gibi birçok faktörün dikkatle ele alınmasını gerektirir.

Soru 2: Öğrenilmiş örüntü nedir?

Hopfield ağları öğrenilmiş örüntüleri hatırlayabilen bir tür sinir ağıdır. Örüntüler, ağın öğrenme aşamasında sunulan veri setleridir ve genellikle bir dizi örnekle ifade edilir. Bu örüntüler, Hopfield ağına sunulduktan sonra, ağın hatırlama işlevi kullanılarak yeniden çağrılabilir.

Örüntüler genellikle, örneğin siyah beyaz bir görüntüdeki piksel değerleri veya bir dizi alfanumerik karakterdeki piksel matrisleri gibi, düşük boyutlu ve sayısal verilerdir. Örüntü öğrenme aşamasında, Hopfield ağına sunulan örüntüler, ağdaki ağırlıkların öğrenme algoritmaları kullanılarak ayarlanması yoluyla, ağın öğrenilmiş örüntüleri saklamasını sağlar.

Öğrenilmiş örüntüler, Hopfield ağının hatırlama işlevi kullanılarak yeniden çağrılabilir. Bu işlem, Hopfield ağının öğrenilmiş örüntüye en yakın olan örüntüyü hatırlamasını ve ağın öğrenilen örüntüye yakın olan diğer örüntüleri de çağırabilmesini sağlar.

\subsection{(8 Puan)} \textbf{ChatGPT’nin açıklamalarından ne anladığınızı buraya kendi kelimelerinizle özetleyin. Örneğin ``Hopfield Network nedir? Ne işe yarar? Neden bilmemiz gerekir bunu? Gerçek hayatta kullanılıyor mu?'' gibi. Anlamadığınız kısımlar varsa ve ChatGPT’nin açıklamaları yeterli gelmezse internet dahil farklı kaynaklardan araştırıp, bilgilerin doğruluğunu sorgulamakta serbestsiniz. Konuyu doğru anlayıp anlamamanız puana etki edecektir. Kullandığınız kaynakları da belirtin!}

Hopfield Network, bozulmaya uğramış şekillerin düzeltilmesinde; el yazı tanıma sistemlerinde; yüz tanıma sistemlerinde; şifreleme işlemlerinde vs kullanılır. Bir tür makine öğrenmesi algoritmasıdır. Elde edilen veriler ne kadar çoksa, bu yazılımın kullanılması sonucu alınan performans da doğru orantılı olarak yüksek olacaktır. Güvenlik sistemlerinde kullanılabilir. Herhangi bir sebepten silinen veriyi, gelişmiş modeli sayesinde tekrar oluşturabilir. Çoklu hücreli yapısı sayesinde iletişim hızlıdır. 

Kaynak: \url{https://medium.com/@batincangurbuz/hopfield-a%C4%9F-modeli-hopfield-network-hn-ccf1548ca432}

\section{(Toplam 10 Puan) Boltzman Machine:}

\subsection{(2 Puan)} \textbf{ChatGPT’ye size Boltzman Machine ilgili detaylı bilgi vermesini isteyin. Sorduğunuz soruyu, ChatGPT'nin çıkartığı cevabı aşağı koyun. Bu cevapta anlamadığınız terimleri de ChatGPT’ye sorup, sorduğunuz soruları ve ChatGPT’nin verdiği cevapları da sırayla aşağı koyun.}

ChatGPT'nin cevabı:

Boltzmann Makinesi, bir tür enerji tabanlı olasılık modelidir ve unsupervised (gözetimsiz) öğrenme için kullanılır. Boltzmann Makinesi, 1980'lerde Geoffrey Hinton ve David Ackley tarafından geliştirilmiştir.

Bir Boltzmann Makinesi, birçok giriş ve çıkış birimi arasındaki bağlantıları içeren bir grafiksel modeldir. Giriş birimleri, verilerin girdisi olarak işlev görürken, çıkış birimleri, modelin tahminlerini üretir. Bağlantılar, bir ağırlık matrisi aracılığıyla gösterilir ve her bir bağlantının bir ağırlık değeri vardır.

Boltzmann Makinesi, iki tip birimden oluşur: gizli birimler ve görünür birimler. Gizli birimler, veri özelliklerini temsil eden daha yüksek seviyeli özelliklerin oluşumunu sağlayan gizli özelliklerdir. Görünür birimler ise, verilerin kendisini temsil eden daha düşük seviyeli özelliklerdir.

Bir Boltzmann Makinesi'nin eğitimi, belirli bir veri kümesinin örnekleri kullanılarak gerçekleştirilir. Eğitim, ağırlıkların belirli bir enerji fonksiyonu tarafından temsil edilen bir olasılık dağılımını takip eden bir Markov Zinciri Monte Carlo (MCMC) işlemi kullanılarak gerçekleştirilir. Bu, ağın verilen verileri temsil etmek için uygun hale gelene kadar ağırlıkların değiştirilmesi anlamına gelir.

Boltzmann Makinesi'nin kullanım alanları, özellikle sinir ağları ve derin öğrenme alanında, boyut indirgeme, sınıflandırma, özellik çıkarma ve veri sıkıştırma gibi görevlerde yaygın olarak kullanılmaktadır.

Soru 1: Gizli ve görünür birimler hakkında daha fazla bilgi verir misin?

Gizli ve görünür birimler, Boltzmann Makinesi'nin temel yapı taşlarıdır ve birçok makine öğrenmesi modelinde kullanılırlar.

Görünür birimler, modelin girdilerini temsil eden ve verilerin düşük seviyeli özelliklerini yakalayan birimlerdir. Görünür birimler, genellikle sayısal değerler içeren bir vektörle temsil edilir. Örneğin, bir resimdeki her bir piksel bir görünür birim olabilir.

Gizli birimler, verilerin yüksek seviyeli özelliklerini temsil eden birimlerdir. Bu özellikler, görünür birimlerin bir araya gelmesi yoluyla elde edilir ve verilerin daha yüksek seviyeli özelliklerini yakalamaya yardımcı olur. Gizli birimlerin varlığı, modelin öğrenme kapasitesini artırır ve modelin verilerin doğal yapısını yakalamasına izin verir.

Bir Boltzmann Makinesi'de, gizli birimler ve görünür birimler arasında bağlantılar vardır. Bu bağlantılar, bir ağırlık matrisi şeklinde gösterilir ve her bir bağlantının bir ağırlık değeri vardır. Bu ağırlıklar, modelin öğrenmesini ve tahminlerini yapmasını sağlar.

Bir Boltzmann Makinesi, öğrenme sırasında gizli birimlerin aktivasyonlarını ve görünür birimlerin aktivasyonlarını dönüşümlü olarak hesaplar. Bu işlem, modelin verilen verileri temsil eden olasılık dağılımını öğrenmesine yardımcı olur.

Gizli ve görünür birimlerin varlığı, Boltzmann Makinesi'nin boyut indirgeme, özellik çıkarma, sınıflandırma ve veri sıkıştırma gibi birçok makine öğrenmesi görevinde kullanılmasını mümkün kılar.

\subsection{(8 Puan)} \textbf{ChatGPT’nin açıklamalarından ne anladığınızı buraya kendi kelimelerinizle özetleyin. Örneğin ``Boltzman Machine nedir? Ne işe yarar? Neden bilmemiz gerekir bunu? Gerçek hayatta kullanılıyor mu?'' gibi. Anlamadığınız kısımlar varsa ve ChatGPT’nin açıklamaları yeterli gelmezse internet dahil farklı kaynaklardan araştırıp, bilgilerin doğruluğunu sorgulamakta serbestsiniz. Konuyu doğru anlayıp anlamamanız puana etki edecektir. Kullandığınız kaynakları da belirtin!}

Boltzman Machine, Hopfield Network ağlarının daha çok olasılıklar ile modellenmiş halidir. Komplike sorunları çözebilirler. İstatikleri, olasılıkları hesaplamak için kullanır. Örnek olarak bir zar 10 kez atıldığında kaç kez 5 rakamının geleceği olasılığını bu makineyi kullanarak öğrenebiliriz. Ancak daha zor konularda da sonuç üretebilir. Hava durumu tahmini gibi zor olasılık tahminlerinde kullanılabilir. Grafiksel bir modeldir. Bu model, gizli ve görünür adı verilen birimlerden oluşur. Görünür birimler, girdilerdir. Gizli birimler ise verilerin yüksek seviyeli özellikleridir. Bu birimlerin varlığı, modelin öğrenme yetisini arttırır. Tahminleri doğruya daha yakın yapmasını sağlar.

Kaynak: \url{https://en.wikipedia.org/wiki/Boltzmann_machine}

\section{(Toplam 10 Puan) Markov Assumption ve Markov Chain:}

\subsection{(2 Puan)} \textbf{ChatGPT’ye size Markov assumption ve Markov Chain ilgili detaylı bilgi vermesini isteyin. Sorduğunuz soruyu, ChatGPT'nin çıkartığı cevabı aşağı koyun. Bu cevapta anlamadığınız terimleri de ChatGPT’ye sorup, sorduğunuz soruları ve ChatGPT’nin verdiği cevapları da sırayla aşağı koyun.}

ChatGPT'nin cevabı: 

Markov Assumption, bir olayın sadece doğrudan önceki olaya bağlı olduğunu varsayan bir olasılık teorisidir. Bu varsayıma göre, olayın öncesindeki geçmiş durumlar ya da olaylar, olayın gelecekteki durumunu etkilemez. Bu varsayım, Markov Chain (Markov Zinciri) olarak bilinen bir olasılık sürecini modellemek için kullanılır.

Markov Chain, belirli bir durumdan başlayarak zaman içinde diğer durumlara geçen bir süreci temsil eden bir matematiksel modeldir. Bu süreç, herhangi bir zamandaki durumun sadece doğrudan önceki duruma bağlı olduğu Markov Assumption varsayımına dayanır. Markov Chain, rastgele değişkenlerin zaman içindeki davranışlarını ve olasılıklarını modellerken kullanılır.

Markov Chain'in temel bileşenleri durumlar, geçiş olasılıkları ve başlangıç dağılımıdır. Durumlar, sürecin geçebileceği farklı olası durumları temsil eder. Geçiş olasılıkları, bir durumdan diğer duruma geçme olasılıklarını gösterir. Başlangıç dağılımı, sürecin başladığı durumun olasılık dağılımını belirtir.

Markov Chain, birçok alanda kullanılır. Örneğin, doğal dil işlemede, kelime sıralarını ve olasılıklarını modellemek için kullanılır. Ekonomide, hisse senetleri fiyatlarını ve faiz oranlarını modellerken kullanılır. Epidemiolojide, hastalık yayılımını ve enfeksiyon oranlarını modellerken kullanılır.

Soru 1: Epidemioloji nedir?

Epidemioloji, hastalıkların ve sağlıkla ilgili olayların yayılımını ve dağılımını inceleyen, nedenlerini araştıran ve sağlık politikalarının geliştirilmesine katkıda bulunan bir bilim dalıdır. Hastalık ve sağlık olaylarının epidemiyolojik analizi, toplum sağlığı için önemli bir rol oynar.

Epidemiolojik araştırmalar, birçok farklı disiplinin bir araya geldiği, veri analizi ve istatistik yöntemlerinin kullanıldığı çok yönlü çalışmalardır. Bu araştırmalar, bir hastalığın yayılımını ve nedenlerini anlamaya, hastalığın önlenmesi ve kontrol altına alınması için stratejiler geliştirmeye, halk sağlığı politikalarının belirlenmesine ve sağlık hizmetlerinin planlanmasına katkıda bulunmaya yöneliktir.

Epidemiolojik araştırmalar, hastalıkların yayılımını ve nedenlerini belirlemek için farklı yöntemler kullanır. Bu yöntemler arasında epidemiyolojik araştırmalar, vaka-kontrol çalışmaları, kohort çalışmaları, çapraz kesit çalışmaları ve randomize kontrollü çalışmalar yer alır. Bu yöntemler, hastalıkların nedenlerini ve yayılımını anlamaya ve sağlık politikalarının geliştirilmesine katkıda bulunmaya yönelik veriler sağlar.

\subsection{(8 Puan)} \textbf{ChatGPT’nin açıklamalarından ne anladığınızı buraya kendi kelimelerinizle özetleyin. Örneğin ``Markov assumption ve Markov Chain nedir? Ne işe yarar? Neden bilmemiz gerekir bunu? Gerçek hayatta kullanılıyor mu?'' gibi. Anlamadığınız kısımlar varsa ve ChatGPT’nin açıklamaları yeterli gelmezse internet dahil farklı kaynaklardan araştırıp, bilgilerin doğruluğunu sorgulamakta serbestsiniz. Konuyu doğru anlayıp anlamamanız puana etki edecektir. Kullandığınız kaynakları da belirtin!}

Markov Chain, Markov Assumption'ı olasılık sürecini modellemede kullanır. Markov Assumption'da bir olay, kendinden sadece bir önceki olaya bağlı olabilir yaklaşımı uygulanmıştır. Bu şekilde bir olaylar zinciri düşünürsek, her bir olay kendinden önceki olaya bağlı olmuş olur. Bu olayların her birinin bağlı olduğu olay ile arasındaki farklardan yola çıkılarak Markov zinciri oluşturulmuş olur. Bir değişkenin zaman içindeki davranışı bu şekilde bir olasılığa bağlanabilir. Gerçek hayatta kullanılır. Ekonomi, sağlık alanları kullanıldığı alanlara örnek verilebilir.

\section{(Toplam 20 Puan) Feed Forward:}
 
\begin{itemize}
    \item Forward propagation için, input olarak şu X matrisini verin (tensöre çevirmeyi unutmayın):\\
    $X = \begin{bmatrix}
        1 & 2 & 3\\
        4 & 5 & 6
        \end{bmatrix}$
    Satırlar veriler (sample'lar), kolonlar öznitelikler (feature'lar).
    \item Bir adet hidden layer olsun ve içinde tanh aktivasyon fonksiyonu olsun
    \item Hidden layer'da 50 nöron olsun
    \item Bir adet output layer olsun, tek nöronu olsun ve içinde sigmoid aktivasyon fonksiyonu olsun
\end{itemize}

Tanh fonksiyonu:\\
$f(x) = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}$
\vspace{.2in}

Sigmoid fonksiyonu:\\
$f(x) = \frac{1}{1 + exp(-x)}$

\vspace{.2in}
 \textbf{Pytorch kütüphanesi ile, ama kütüphanenin hazır aktivasyon fonksiyonlarını kullanmadan, formülünü verdiğim iki aktivasyon fonksiyonunun kodunu ikinci haftada yaptığımız gibi kendiniz yazarak bu yapay sinir ağını oluşturun ve aşağıdaki üç soruya cevap verin.}
 
\subsection{(10 Puan)} \textbf{Yukarıdaki yapay sinir ağını çalıştırmadan önce pytorch için Seed değerini 1 olarak set edin, kodu aşağıdaki kod bloğuna ve altına da sonucu yapıştırın:}

% Latex'de kod koyabilirsiniz python formatında. Aşağıdaki örnekleri silip içine kendi kodunuzu koyun
\begin{python}
kod_buraya = None
if kod_buraya:
    devam_ise_buraya = 0

print(devam_ise_buraya)
\end{python}

Sonuç da buraya

\subsection{(5 Puan)} \textbf{Yukarıdaki yapay sinir ağını çalıştırmadan önce Seed değerini öğrenci numaranız olarak değiştirip, kodu aşağıdaki kod bloğuna ve altına da sonucu yapıştırın:}

\begin{python}
kod_buraya = None
if kod_buraya:
    devam_ise_buraya = 0

print(devam_ise_buraya)
\end{python}

Sonuç da buraya

\subsection{(5 Puan)} \textbf{Kodlarınızın ve sonuçlarınızın olduğu jupyter notebook'un Github repository'sindeki linkini aşağıdaki url kısmının içine yapıştırın. İlk sayfada belirttiğim gün ve saate kadar halka açık (public) olmasın:}
% size ait Github olmak zorunda, bu vize için ayrı bir github repository'si açıp notebook'u onun içine koyun. Kendine ait olmayıp da arkadaşının notebook'unun linkini paylaşanlar 0 alacak.

\url{www.githublinkiburaya.com}

\section{(Toplam 40 Puan) Multilayer Perceptron (MLP):} 
\textbf{Bu bölümdeki sorularda benim vize ile beraber paylaştığım Prensesi İyileştir (Cure The Princess) Veri Seti parçaları kullanılacak. Hikaye şöyle (soruyu çözmek için hikaye kısmını okumak zorunda değilsiniz):} 

``Bir zamanlar, çok uzaklarda bir ülkede, ağır bir hastalığa yakalanmış bir prenses yaşarmış. Ülkenin kralı ve kraliçesi onu iyileştirmek için ellerinden gelen her şeyi yapmışlar, ancak denedikleri hiçbir çare işe yaramamış.

Yerel bir grup köylü, herhangi bir hastalığı iyileştirmek için gücü olduğu söylenen bir dizi sihirli malzemeden bahsederek kral ve kraliçeye yaklaşmış. Ancak, köylüler kral ile kraliçeyi, bu malzemelerin etkilerinin patlayıcı olabileceği ve son zamanlarda yaşanan kuraklıklar nedeniyle bu malzemelerden sadece birkaçının herhangi bir zamanda bulunabileceği konusunda uyarmışlar. Ayrıca, sadece deneyimli bir simyacı bu özelliklere sahip patlayıcı ve az bulunan malzemelerin belirli bir kombinasyonunun prensesi iyileştireceğini belirleyebilecekmiş.

Kral ve kraliçe kızlarını kurtarmak için umutsuzlar, bu yüzden ülkedeki en iyi simyacıyı bulmak için yola çıkmışlar. Dağları tepeleri aşmışlar ve nihayet "Yapay Sinir Ağları Uzmanı" olarak bilinen yeni bir sihirli sanatın ustası olarak ün yapmış bir simyacı bulmuşlar.

Simyacı önce köylülerin iddialarını ve her bir malzemenin alınan miktarlarını, ayrıca iyileşmeye yol açıp açmadığını incelemiş. Simyacı biliyormuş ki bu prensesi iyileştirmek için tek bir şansı varmış ve bunu doğru yapmak zorundaymış. (Original source: \url{https://www.kaggle.com/datasets/unmoved/cure-the-princess})

(Buradan itibaren ChatGPT ve Dr. Ulya Bayram'a ait hikayenin devamı)

Simyacı, büyülü bileşenlerin farklı kombinasyonlarını analiz etmek ve denemek için günler harcamış. Sonunda birkaç denemenin ardından prensesi iyileştirecek çeşitli karışım kombinasyonları bulmuş ve bunları bir veri setinde toplamış. Daha sonra bu veri setini eğitim, validasyon ve test setleri olarak üç parçaya ayırmış ve bunun üzerinde bir yapay sinir ağı eğiterek kendi yöntemi ile prensesi iyileştirme ihtimalini hesaplamış ve ikna olunca kral ve kraliçeye haber vermiş. Heyecanlı ve umutlu olan kral ve kraliçe, simyacının prensese hazırladığı ilacı vermesine izin vermiş ve ilaç işe yaramış ve prenses hastalığından kurtulmuş.

Kral ve kraliçe, kızlarının hayatını kurtardığı için simyacıya krallıkta kalması ve çalışmalarına devam etmesi için büyük bir araştırma bütçesi ve çok sayıda GPU'su olan bir server vermiş. İyileşen prenses de kendisini iyileştiren yöntemleri öğrenmeye merak salıp, krallıktaki üniversitenin bilgisayar mühendisliği bölümüne girmiş ve mezun olur olmaz da simyacının yanında, onun araştırma grubunda çalışmaya başlamış. Uzun yıllar birlikte krallıktaki insanlara, hayvanlara ve doğaya faydalı olacak yazılımlar geliştirmişler, ve simyacı emekli olduğunda prenses hem araştırma grubunun hem de krallığın lideri olarak hayatına devam etmiş.

Prenses, kendisini iyileştiren veri setini de, gelecekte onların izinden gidecek bilgisayar mühendisi prensler ve prensesler başkalarına faydalı olabilecek yapay sinir ağları oluşturmayı öğrensinler diye halka açmış ve sınavlarda kullanılmasını salık vermiş.''

\textbf{İki hidden layer'lı bir Multilayer Perceptron (MLP) oluşturun beşinci ve altıncı haftalarda yaptığımız gibi. Hazır aktivasyon fonksiyonlarını kullanmak serbest. İlk hidden layer'da 100, ikinci hidden layer'da 50 nöron olsun. Hidden layer'larda ReLU, output layer'da sigmoid aktivasyonu olsun.}

\textbf{Output layer'da kaç nöron olacağını veri setinden bakıp bulacaksınız. Elbette bu veriye uygun Cross Entropy loss yöntemini uygulayacaksınız. Optimizasyon için Stochastic Gradient Descent yeterli. Epoch sayınızı ve learning rate'i validasyon seti üzerinde denemeler yaparak (loss'lara overfit var mı diye bakarak) kendiniz belirleyeceksiniz. Batch size'ı 16 seçebilirsiniz.}

\subsection{(10 Puan)} \textbf{Bu MLP'nin pytorch ile yazılmış class'ının kodunu aşağı kod bloğuna yapıştırın:}

\begin{python}
class MLP(nn.Module):

    def __init__(self, inputSize):

        super(MLP, self).__init__()
        self.hiddenLayer1 = nn.Linear(inputSize, 100)
        self.relu1 = nn.ReLU()
        self.hiddenLayer2 = nn.Linear(100, 50)
        self.relu2 = nn.ReLU()
        self.outputLayer = nn.Linear(50, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):

        out = self.hiddenLayer1(x)
        out = self.relu1(out)
        out = self.hiddenLayer2(out)
        out = self.relu2(out)
        out = self.outputLayer(out)
        out = self.sigmoid(out)
        return out

\end{python}

\subsection{(10 Puan)} \textbf{SEED=öğrenci numaranız set ettikten sonra altıncı haftada yazdığımız gibi training batch'lerinden eğitim loss'ları, validation batch'lerinden validasyon loss değerlerini hesaplayan kodu aşağıdaki kod bloğuna yapıştırın ve çıkan figürü de alta ekleyin.}

\begin{python}
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(180401058)

class MLP(nn.Module):

    def __init__(self, inputSize):

        super(MLP, self).__init__()
        self.hiddenLayer1 = nn.Linear(inputSize, 100)
        self.relu1 = nn.ReLU()
        self.hiddenLayer2 = nn.Linear(100, 50)
        self.relu2 = nn.ReLU()
        self.outputLayer = nn.Linear(50, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):

        out = self.hiddenLayer1(x)
        out = self.relu1(out)
        out = self.hiddenLayer2(out)
        out = self.relu2(out)
        out = self.outputLayer(out)
        out = self.sigmoid(out)
        return out

###############################
#buraya kadar soru 5.1
###############################

class CustomDataset(Dataset):

    def __init__(self, csv_file):

        self.data = pd.read_csv(csv_file)
        self.x = self.data.iloc[:, :-1].values
        self.y = self.data.iloc[:, -1].values.reshape(-1, 1)

    def __getitem__(self, idx):
        return torch.Tensor(self.x[idx]), torch.Tensor(self.y[idx])

    def __len__(self):
        return len(self.data)

trainDataset = CustomDataset('/home/basak/indirilenler/cure_the_princess_train.csv')
trainLoader = DataLoader(trainDataset, batch_size=16, shuffle=True)

testDataset = CustomDataset('/home/basak/indirilenler/cure_the_princess_test.csv')
testLoader = DataLoader(testDataset, batch_size=16, shuffle=False)

valDataset = CustomDataset('/home/basak/indirilenler/cure_the_princess_validation.csv')
valLoader = DataLoader(valDataset, batch_size=16, shuffle=False)

model = MLP(inputSize=13)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
minValLoss = 1

for epoch in range(20):

    for i, data in enumerate(trainLoader):

        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    totalValLoss = 0.0
    for i, data in enumerate(valLoader):

        inputs, labels = data
        outputs = model(inputs)
        valLoss = criterion(outputs, labels)
        totalValLoss += valLoss.item()

    print('Epoch: %d \nValidation Loss: %.3f' % (epoch+1, totalValLoss/len(valLoader)))

total_test_loss = 0.0
correct = 0
total = 0

with torch.no_grad():

    for i, data in enumerate(testLoader):

        inputs, labels = data
        outputs = model(inputs)
        test_loss = criterion(outputs, labels)
        total_test_loss += test_loss.item()
        predicted = torch.round(outputs)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print('Totals : Test Loss: %.3f, Test Accuracy: %.3f' % (total_test_loss/len(testLoader), correct/total))

\end{python}
\begin{center}
    \centering
    \includegraphics{resim2.png}
    %\captionof{Soru 5.2}
    \label{fig:my_pic}
\end{center}


\subsection{(10 Puan)} \textbf{SEED=öğrenci numaranız set ettikten sonra altıncı haftada ödev olarak verdiğim gibi earlystopping'deki en iyi modeli kullanarak, Prensesi İyileştir test setinden accuracy, F1, precision ve recall değerlerini hesaplayan kodu yazın ve sonucu da aşağı yapıştırın. \%80'den fazla başarı bekliyorum test setinden. Daha düşükse başarı oranınız, nerede hata yaptığınızı bulmaya çalışın. \%90'dan fazla başarı almak mümkün (ben denedim).}

\begin{python}
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

torch.manual_seed(180401058)

class MLP(nn.Module):

    def __init__(self, inputSize):

        super(MLP, self).__init__()
        self.hiddenLayer1 = nn.Linear(inputSize, 100)
        self.relu1 = nn.ReLU()
        self.hiddenLayer2 = nn.Linear(100, 50)
        self.relu2 = nn.ReLU()
        self.outputLayer = nn.Linear(50, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):

        out = self.hiddenLayer1(x)
        out = self.relu1(out)
        out = self.hiddenLayer2(out)
        out = self.relu2(out)
        out = self.outputLayer(out)
        out = self.sigmoid(out)
        return out

###############################
#buraya kadar soru 5.1
###############################

class CustomDataset(Dataset):

    def __init__(self, csv_file):

        self.data = pd.read_csv(csv_file)
        self.x = self.data.iloc[:, :-1].values
        self.y = self.data.iloc[:, -1].values.reshape(-1, 1)

    def __getitem__(self, idx):
        return torch.Tensor(self.x[idx]), torch.Tensor(self.y[idx])

    def __len__(self):
        return len(self.data)


trainDataset = CustomDataset('/home/basak/indirilenler/cure_the_princess_train.csv')
trainLoader = DataLoader(trainDataset, batch_size=16, shuffle=True)

testDataset = CustomDataset('/home/basak/indirilenler/cure_the_princess_test.csv')
testLoader = DataLoader(testDataset, batch_size=16, shuffle=False)

valDataset = CustomDataset('/home/basak/indirilenler/cure_the_princess_validation.csv')
valLoader = DataLoader(valDataset, batch_size=16, shuffle=False)

model = MLP(inputSize=13)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
minValLoss = 1

for epoch in range(20):

    for i, data in enumerate(trainLoader):

        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    totalValLoss = 0.0
    for i, data in enumerate(valLoader):

        inputs, labels = data
        outputs = model(inputs)
        valLoss = criterion(outputs, labels)
        totalValLoss += valLoss.item()
    if((totalValLoss/len(valLoader)) < minValLoss):

        minValLoss = (totalValLoss/len(valLoader))
        torch.save(model.state_dict(), 'model.pt')

total_test_loss = 0.0
correct = 0
total = 0

with torch.no_grad():

    for i, data in enumerate(testLoader):

        inputs, labels = data
        outputs = model(inputs)
        test_loss = criterion(outputs, labels)
        total_test_loss += test_loss.item()
        predicted = torch.round(outputs)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

###############################################
#buraya kadarki kisim soru 5.2
###############################################

bestModel = MLP(inputSize=13)
bestModel.load_state_dict(torch.load("model.pt"))

testDataset = CustomDataset('/home/basak/indirilenler/cure_the_princess_test.csv')
testLoader = DataLoader(testDataset, batch_size=16, shuffle=False)

with torch.no_grad():

    trueLabels = []
    preLabels = []

    for i, data in enumerate(testLoader):

        inputs, labels = data
        outputs = bestModel(inputs)
        predicted = torch.round(outputs)
        trueLabels += labels.squeeze().tolist()
        preLabels += predicted.squeeze().tolist()

accuracy = accuracy_score(trueLabels, preLabels)
f1 = f1_score(trueLabels, preLabels)
precision = precision_score(trueLabels, preLabels)
recall = recall_score(trueLabels, preLabels)

print('Accuracy: %.3f \nF1: %.3f \nPrecision: %.3f \nRecall: %.3f' % (accuracy, f1, precision, recall))
\end{python}

\begin{center}
    \centering
    \includegraphics{resim3.png}
    \label{fig:my_pic}
\end{center}

\subsection{(5 Puan)} \textbf{Tüm kodların CPU'da çalışması ne kadar sürüyor hesaplayın. Sonra to device yöntemini kullanarak modeli ve verileri GPU'ya atıp kodu bir de böyle çalıştırın ve ne kadar sürdüğünü hesaplayın. Süreleri aşağıdaki tabloya koyun. GPU için Google Colab ya da Kaggle'ı kullanabilirsiniz, iki ortam da her hafta saatlerce GPU hakkı veriyor.}

\begin{table}[ht!]
    \centering
    \caption{Buraya bir açıklama yazın}
    \begin{tabular}{c|c}
        Ortam & Süre (saniye) \\\hline
        CPU & kaç? \\
        GPU & kaç?\\
    \end{tabular}
    \label{tab:my_table}
\end{table}

\subsection{(3 Puan)} \textbf{Modelin eğitim setine overfit etmesi için elinizden geldiği kadar kodu gereken şekilde değiştirin, validasyon loss'unun açıkça yükselmeye başladığı, training ve validation loss'ları içeren figürü aşağı koyun ve overfit için yaptığınız değişiklikleri aşağı yazın. Overfit, tam bir çanak gibi olmalı ve yükselmeli. Ona göre parametrelerle oynayın.}

Cevaplar buraya

% Figür aşağı
\begin{comment}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.75\textwidth]{mypicturehere.png}
    \caption{Buraya açıklama yazın}
    \label{fig:my_pic}
\end{figure}
\end{comment}

\subsection{(2 Puan)} \textbf{Beşinci soruya ait tüm kodların ve cevapların olduğu jupyter notebook'un Github linkini aşağıdaki url'e koyun.}

\url{www.benimgithublinkim.com}

\section{(Toplam 10 Puan)} \textbf{Bir önceki sorudaki Prensesi İyileştir problemindeki yapay sinir ağınıza seçtiğiniz herhangi iki farklı regülarizasyon yöntemi ekleyin ve aşağıdaki soruları cevaplayın.} 

\subsection{(2 puan)} \textbf{Kodlarda regülarizasyon eklediğiniz kısımları aşağı koyun:} 

\begin{python}
kod_buraya = None
if kod_buraya:
    devam_ise_buraya = 0

print(devam_ise_buraya)
\end{python}

\subsection{(2 puan)} \textbf{Test setinden yeni accuracy, F1, precision ve recall değerlerini hesaplayıp aşağı koyun:}

Sonuçlar buraya.

\subsection{(5 puan)} \textbf{Regülarizasyon yöntemi seçimlerinizin sebeplerini ve sonuçlara etkisini yorumlayın:}

Yorumlar buraya.

\subsection{(1 puan)} \textbf{Sonucun github linkini  aşağıya koyun:}

\url{www.benimgithublinkim2.com}

\end{document}
